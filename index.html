<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Supplementary Materials - VIAS</title>
  <style>
    body {
      font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f9f9f9;
      color: #222;
      line-height: 1.6;
    }

    header {
      background-color: #003366;
      color: white;
      padding: 2rem 1rem;
      text-align: center;
    }

    header h1 {
      margin: 0;
      font-size: 2rem;
      font-weight: 600;
    }

    main {
      max-width: 900px;
      margin: 2rem auto;
      padding: 0 1rem;
      background-color: white;
      border-radius: 8px;
      box-shadow: 0 0 10px rgba(0,0,0,0.1);
    }

    section {
      margin-bottom: 3rem;
    }

    section h2 {
      border-left: 4px solid #003366;
      padding-left: 10px;
      color: #003366;
      font-size: 1.4rem;
      margin-bottom: 0.5rem;
    }

    p {
      font-size: 1rem;
      text-align: justify;
    }

    iframe {
      display: block;
      margin: 1rem auto;
      max-width: 100%;
      border-radius: 8px;
      border: none;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      margin-top: 1rem;
      margin-bottom: 1rem;
      font-size: 0.95rem;
    }

    th, td {
      border: 1px solid #ddd;
      padding: 8px 10px;
      text-align: center;
    }

    th {
      background-color: #003366;
      color: white;
    }

    /* 图片基础样式 */
    img {
      display: block;
      border-radius: 6px;
    }

    .caption {
      text-align: center;
      font-size: 0.9rem;
      color: #555;
      margin-top: 10px;
      margin-bottom: 0;
    }

    /* 左右分栏布局 */
    .two-column-layout {
      display: flex;
      gap: 2rem;
      margin: 1.5rem 0;
      align-items: flex-start;
    }

    .image-column {
      flex: 1;
      min-width: 0;
      display: flex;
      flex-direction: column;
      align-items: center;
    }

    .prompt-column {
      flex: 1;
      min-width: 0;
    }

    .explanation-column {
      flex: 1;
      min-width: 0;
    }

    /* 特定图片尺寸控制 - 去掉边框 */
    .algo-image {
      width: 100%;
      max-width: 350px;
      height: auto;
      border: none; /* 去掉边框 */
    }

    .para-image {
      width: 100%;
      max-width: 400px;
      height: auto;
      border: none; /* 去掉边框 */
    }

    .normal-image {
      width: 100%;
      max-width: 100%;
      height: auto;
      border: none; /* 去掉边框 */
    }

    /* Prompt内容样式 */
    .prompt-content {
      background-color: #f9f9f9;
      border: 1px solid #ccc;
      border-radius: 6px;
      padding: 16px;
      font-family: 'Courier New', monospace;
      white-space: pre-wrap;
      line-height: 1.5;
      overflow-x: auto;
      font-size: 0.9rem;
      max-height: 500px;
      overflow-y: auto;
    }

    .prompt-title {
      font-weight: bold;
      color: #003366;
      margin-bottom: 10px;
      font-size: 1rem;
    }

    /* 解释部分样式 */
    .explanation-content {
      background-color: #f8f9fa;
      border: 1px solid #ddd;
      border-radius: 6px;
      padding: 20px;
      max-height: 500px;
      overflow-y: auto;
    }

    .explanation-content h3 {
      color: #003366;
      margin-top: 0;
      margin-bottom: 1rem;
      font-size: 1.2rem;
    }

    .explanation-point {
      margin-bottom: 1.5rem;
      padding: 15px;
      background-color: white;
      border-radius: 4px;
      border-left: 3px solid #3498db;
    }

    .explanation-point strong {
      color: #2c3e50;
      display: block;
      margin-bottom: 8px;
      font-size: 1.1rem;
    }

    /* 滚动条样式 */
    .explanation-content::-webkit-scrollbar {
      width: 6px;
    }

    .explanation-content::-webkit-scrollbar-track {
      background: #f1f1f1;
      border-radius: 3px;
    }

    .explanation-content::-webkit-scrollbar-thumb {
      background: #3498db;
      border-radius: 3px;
    }

    .explanation-content::-webkit-scrollbar-thumb:hover {
      background: #2980b9;
    }

    footer {
      background-color: #003366;
      color: white;
      text-align: center;
      padding: 1rem;
      font-size: 0.85rem;
    }

    /* 响应式设计 */
    @media (max-width: 768px) {
      .two-column-layout {
        flex-direction: column;
        gap: 1rem;
      }
      
      .image-column,
      .prompt-column,
      .explanation-column {
        width: 100%;
      }
      
      header h1 {
        font-size: 1.5rem;
      }
      
      section h2 {
        font-size: 1.2rem;
      }
      
      .algo-image,
      .para-image {
        max-width: 100%;
      }
      
      .prompt-content,
      .explanation-content {
        max-height: 300px;
      }
    }
  </style>
</head>

<body>
  <header>
    <h1>Initializing and Shaping Robot Policies with Language-based Value Estimation</h1>
    <h2>Appendix</h2>
  </header>

  <main>

    <!-- Abstract -->
    <section>
      <h2>Abstract</h2>
      <p>
        Reinforcement Learning (RL) enables robots to learn complex tasks, but suffers from poor sample efficiency, especially in environments where language is used to describe goals and actions. While recent work has explored the integration of RL with large language models (LLMs), most methods focus on policy learning and overlook the potential of using the knowledge as structured feedback to guide value estimation. We present Value Initialization and Adaptive Shaping (VIAS), a framework that uses large language models as external critics to provide linguistic guidance for value estimation. VIAS enhances sample efficiency by using language for both informed initialization and value shaping during training. Evaluated on two robots and in VirtualHome environment, VIAS outperforms standard RL baselines in both learning speed and task completion, demonstrating its potential for real-world robot applications.
      </p>
    </section>

    <!-- Video -->
    <section>
      <h2>Video Demonstration of Real Robot Experiments with VIAS</h2>
      <iframe width="720" height="405" src="https://www.youtube.com/embed/l7xpJGB_HDQ" allowfullscreen></iframe>
      <p class="caption">Video 1. Demonstration of VIAS performance in robot experiments.</p>
      <p>
        This video demonstrates the learning process and task completion results of Robot completing the goal:"Goal: put plate on kitchentable and then put cup on kitchentablethe" with VIAS framework. To validate the proposed approach in a real-world scenario, we conducted a real-robot experiment using a mobile manipulator tasked with setting up a table according to a specified goal. The mobile manipulator used in this demonstration consists of a Segway base for navigation, a UR5e robotic arm equipped with a Hand-E gripper mounted on the Segway base for manipulation, and an overhead RGB-D camera fixed relative to the robot for perception. This setup provides the robot with the capabilities to perceive its environment, navigate within it, and interact with objects effectively.
      </p>
    </section>

    <!-- VIAS Framework Overview Image -->
    <section>
      <h2>VIAS Algorithm</h2>
      <div class="two-column-layout">
        <div class="image-column">
            <img src="algo.png" alt="VIAS Framework Overview" class="algo-image" />
            <p class="caption">Algorithm 1. Full procedure of VIAS, consisting of an initialization stage where a Q-network is pre-trained using LLM-generated heuristic values, followed by a standard deep Q-learning phase enhanced with ongoing LLM-based shaping.</p>
        </div>
        
        <div class="explanation-column">
            <div class="explanation-content">
                <h3>Algorithm Explanation</h3>
                <p>Algorithm 1 outlines the full procedure for VIAS, consisting of an initialization stage where a Q-network is pre-trained using LLM-generated heuristic values, followed by a standard deep Q-learning phase enhanced with ongoing LLM-based shaping.</p>
                
                <div class="explanation-point">
                    <strong>Initialization Phase (Lines 1–7)</strong>
                    We begin by setting up an empty initial replay buffer D₀. This buffer is used to store observation-action pairs along with their corresponding heuristic values obtained from the LLM before training. For each observation-action pair (o, a) collected before the training starts, the algorithm queries the LLM to obtain a heuristic value f. This heuristic value represents the LLM's estimated cumulative reward for taking action a in observation o. The tuple (o, a, f) is then stored in the initial replay buffer D₀. Subsequently, the algorithm initializes the Q-value function Q(o, a; θ) by minimizing the pre-training loss function L₀(θ).
                </div>

                <div class="explanation-point">
                    <strong>Training Phase (Lines 9–23)</strong>
                    We train the agent over T timesteps. As in the initialization phase, VIAS initializes a replay buffer D. At each time step within an episode, the agent selects an action a based on the current Q-values Q(o, a). This selection is typically performed using a policy derived from Q, such as the ϵ-greedy policy. In our experiment, we used a decayed ϵ-greedy policy over timesteps. The selected action a is then executed in the environment, resulting in the observation of an immediate reward r and the next observation o′. Querying the LLM to obtain a heuristic value f for the current observation-action pair, the agent stores the experience in the replay buffer D. A mini-batch of experiences is then sampled from the replay buffer D for training, using prioritized experience replay. The Q-value Q(o, a) is then updated by incorporating both the TD error and the heuristic adjustment from the LLM. When the agent achieves the goal, the environment is initialized by setting the initial observation o.
                </div>

                <div class="explanation-point">
                    <strong>Key Components</strong>
                    • <strong>LLM Integration:</strong> Uses language models as external critics to provide heuristic values<br>
                    • <strong>Value Shaping:</strong> Augments TD loss with linguistic guidance<br>
                    • <strong>Adaptive Updates:</strong> Periodic LLM queries maintain relevance<br>
                    • <strong>Sample Efficiency:</strong> Informed initialization accelerates learning<br>
                    • <strong>Prioritized Replay:</strong> Focuses learning on valuable experiences
                </div>
            </div>
        </div>
      </div>
    </section>

    <!-- Parameter Analysis Image -->
    <section>
      <h2>TABLE IV KEY HYPERPARAMETERS USED IN VIAS</h2>
      <div class="two-column-layout">
        <div class="image-column">
            <img src="para.png" alt="Parameter Analysis" class="para-image" />
            <p class="caption">Figure 2. Parameter analysis and sensitivity studies of the VIAS framework.</p>
        </div>
        
        <div class="explanation-column">
            <div class="explanation-content">
                <h3>Parameter Sensitivity Analysis</h3>
                <p>Table IV summarizes the key hyperparameters used in our VIAS implementation based on the QVIAS agent. We highlight components that are particularly relevant to the integration of Large Language Model (LLM) feedback and TDQN.</p>
            </div>
        </div>
      </div>
    </section>

    <!-- 3D Visualization Image -->
    <section>
      <h2>Q-value Comparison</h2>
      <img src="3d.png" alt="3D Visualization" class="normal-image" />
      <p class="caption">Three-Dimensional Analysis of Action Selection Policies Across Three Sequential States.</p>
      <p>
        This figure presents a comparative analysis of Q-values generated by different frameworks for various actions under the same state in the VirtualHome environment. The agent’s Goal is: "Goal: put 1 cutleryfork on the kitchentable.” The Consecutive State axis represents three sequential states experienced by the agent under the optimal policy: “Agent is in Bathroom,” “Agent is in Kitchen,” and “Agent is close to Cutleryfork.” The Valid Actions axis enumerates all executable actions available to the agent across these states. The Q-value axis displays the normalized Q-values scaled to the interval [0,1]. We compare the Baseline TDQN—one is in its initial training phase and another one is under the final optimized policy—with an LLM-based approach that directly generates heuristic Q-values for each state–action pair. Actions marked with a triangle on the bars indicate the ones selected by the agent in the corresponding state. The results demonstrate that the TDQN agent in the early training phase exhibits a near-random Q-value distribution and suboptimal action choices. In contrast, the LLM-driven heuristic values show strong alignment with the TDQN’s final optimized policy. This indicates that the large language model can effectively comprehend the task environment and provide reasonable, informed heuristic Q-values, offering robust support for the integration of LLMs in reinforcement learning frameworks.
      </p>
    </section>

    <!-- Q-Value Evaluation Prompt -->
    <section>
      <h2>Q-Value Evaluation Prompt</h2>
      <div class="two-column-layout">
        <!-- 左边：Prompt内容 -->
        <div class="prompt-column">
          <div class="prompt-content">
            <div class="prompt-title">Q-Value Evaluation Prompt</div>
"Your task is to estimate Q-values. Given a list of different observations and corresponding possible actions, please evaluate the Q-value of each action. The Q-value represents the expected cumulative reward obtained by taking that action in the current state and following the optimal policy thereafter. Please return a numeric score between 0.0 and 1.0 for each action, where:
- 1.0 indicates the highest possible expected utility toward achieving the goal.
- 0.0 indicates no expected utility or a clearly wrong move.
Return the values in dictionary format. Only return numeric values between 0.0 and 1.0. If an action appears nonsensical, assign it 0.0.

You may use commonsense spatial reasoning. If a goal involves placing an object on a specific surface, you can infer which intermediate steps (e.g., walking to, grabbing) are needed to accomplish the goal. An object must be near you to grab it. You must be holding an object to put it back on another surface.

[VALID ACTIONS]
- walk cutleryfork
- walk wineglass
- walk plate
- walk kitchentable
- walk kitchencounter
- walk kitchen
- walk bathroom
- walk bedroom
- walk livingroom
- grab cutleryfork
- grab wineglass
- grab plate
- putback cutleryfork kitchencounter
- putback wineglass kitchencounter
- putback plate kitchencounter
- putback cutleryfork kitchentable
- putback wineglass kitchentable
- putback plate kitchentable

{EXAMPLES}
Now generate Q values:
{OBSERVATION TEMPLATE}
Q values:"

<div class="prompt-title" style="margin-top: 20px;">Observation Template</div>
"[OBSERVATION]
Goal: put 1 {obj1} on {surface}, put 1 {obj1} on {surface}.
You are in {room}.
You see {reachable objects}.
You are close to {reachable objects}.
You are holding {grabable objects}.
{grabable object} is on the {surface}.
Previous Action: {action}"

<div class="prompt-title" style="margin-top: 20px;">Example Response</div>
"{
"walk cutleryfork": 0.2,
"walk wineglass": 0.2,
"walk plate": 0.0,
"walk kitchentable": 0.8,
"walk kitchencounter": 0.1,
"walk kitchen": 0.0,
"walk bathroom": 0.0,
"walk bedroom": 0.0,
"walk livingroom": 0.0,
"grab cutleryfork": 1.0,
"grab wineglass": 1.0,
"grab plate": 0.0,
"putback cutleryfork kitchencounter": 0.0,
"putback wineglass kitchencounter": 0.0,
"putback plate kitchencounter": 0.0,
"putback cutleryfork kitchentable": 0.0,
"putback wineglass kitchentable": 0.0,
"putback plate kitchentable": 0.0
}"
          </div>
        </div>
        
        <!-- 右边：解释 -->
        <div class="explanation-column">
          <div class="explanation-content">
            <h3>Q-Value Evaluation Prompt</h3>
            <p>We design a prompt in a way that the LLM can output estimated cumulative rewards instead of immediate rewards. The prompt also takes advantage of a natural language description of an environment, where the description can be one that the agent has not observed.</p>
            
          </div>
        </div>
      </div>
    </section>

  
  </main>

  <footer>
    &copy; 2025 VIAS Research Team — Supplementary Materials for ICRA Submission
  </footer>
</body>
</html>
